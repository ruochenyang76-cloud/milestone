{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5ce92d-b2c9-4f41-933d-732bdf4c12a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading datasets...\n",
      "Reading all_pull_request.parquet...\n",
      "Loaded PR Data: (932791, 14) rows\n",
      "Reading all_repository.parquet...\n",
      "Loaded Repo Data: (116211, 7) rows\n",
      "Step 2: Merging datasets...\n",
      "Common columns found: ['id']\n",
      "Merging on 'id'...\n",
      "Merged shape: (932791, 20)\n",
      "Step 3: Cleaning & Feature Engineering...\n",
      "Success! Cleaned data saved to: cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_and_clean_data(pr_file_path, repo_file_path, output_path):\n",
    "    print(\"Step 1: Loading datasets...\")\n",
    "    \n",
    "    # 1. Load two Parquet files\n",
    "    try:\n",
    "        # Core Data: PR records (contains Agent, State, Body, Time)\n",
    "        print(f\"Reading {pr_file_path}...\")\n",
    "        df_pr = pd.read_parquet(pr_file_path)\n",
    "        print(f\"Loaded PR Data: {df_pr.shape} rows\")\n",
    "        \n",
    "        # Auxiliary Data: Repository info (contains Language, Stars)\n",
    "        print(f\"Reading {repo_file_path}...\")\n",
    "        df_repo = pd.read_parquet(repo_file_path)\n",
    "        print(f\"Loaded Repo Data: {df_repo.shape} rows\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please ensure the .parquet files are in the same directory as this script.\")\n",
    "        return\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. Merge Datasets\n",
    "    # ==========================================\n",
    "    print(\"Step 2: Merging datasets...\")\n",
    "    \n",
    "    # Automatically find columns to merge on (usually repo_name or repo_id)\n",
    "    common_cols = list(set(df_pr.columns) & set(df_repo.columns))\n",
    "    print(f\"Common columns found: {common_cols}\")\n",
    "    \n",
    "    if not common_cols:\n",
    "        print(\"Error: No common columns to merge on!\")\n",
    "        return\n",
    "        \n",
    "    # Merge on the first common column found\n",
    "    merge_key = 'repo_name' if 'repo_name' in common_cols else common_cols[0]\n",
    "    print(f\"Merging on '{merge_key}'...\")\n",
    "    \n",
    "    # Merge! (Left Join: Keep all PR records, attach corresponding Repo info)\n",
    "    df = pd.merge(df_pr, df_repo, on=merge_key, how='left')\n",
    "    print(f\"Merged shape: {df.shape}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. Data Cleaning & Feature Engineering\n",
    "    # ==========================================\n",
    "    print(\"Step 3: Cleaning & Feature Engineering...\")\n",
    "\n",
    "    # A. Filtering\n",
    "    # We must have Agent Name and PR State for the analysis.\n",
    "    target_cols = ['agent_name', 'state', 'created_at', 'closed_at']\n",
    "    \n",
    "    existing_cols = [c for c in target_cols if c in df.columns]\n",
    "    df = df.dropna(subset=existing_cols)\n",
    "\n",
    "    # B. Feature Engineering: Time-to-Decision\n",
    "    if 'created_at' in df.columns and 'closed_at' in df.columns:\n",
    "        # Convert to datetime objects\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "        df['closed_at'] = pd.to_datetime(df['closed_at'], errors='coerce')\n",
    "        \n",
    "        # Calculate duration in hours\n",
    "        df['decision_time_hours'] = (df['closed_at'] - df['created_at']).dt.total_seconds() / 3600\n",
    "        \n",
    "        # Filter out invalid times\n",
    "        df = df[df['decision_time_hours'] >= 0]\n",
    "\n",
    "    # C. Feature Engineering: Word Count\n",
    "    if 'body' in df.columns:\n",
    "        df['body_word_count'] = df['body'].astype(str).fillna(\"\").apply(lambda x: len(x.split()))\n",
    "\n",
    "    # D. Feature Engineering: Binary State Encoding\n",
    "    if 'state' in df.columns:\n",
    "        df['is_merged'] = df['state'].astype(str).str.lower().apply(lambda x: 1 if 'merged' in x else 0)\n",
    "\n",
    "    # E. Feature Engineering: Language Binning\n",
    "    if 'language' in df.columns:\n",
    "        top_langs = df['language'].value_counts().nlargest(10).index\n",
    "        df['language_grouped'] = df['language'].apply(lambda x: x if x in top_langs else 'Other')\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. Save Results\n",
    "    # ==========================================\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Success! Cleaned data saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define file paths (assuming files are in the CURRENT directory)\n",
    "    pr_file = 'all_pull_request.parquet'\n",
    "    repo_file = 'all_repository.parquet'\n",
    "    output_file = 'cleaned_data.csv'\n",
    "    \n",
    "    load_and_clean_data(pr_file, repo_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488d694-24af-4e07-ae29-0924a509c803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
